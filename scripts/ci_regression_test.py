#!/usr/bin/env python3
"""
Cinema Pro HDR CIå›å½’æµ‹è¯•è„šæœ¬
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ç”Ÿæˆçš„èµ„äº§è¿›è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•
"""

import json
import pandas as pd
import numpy as np
from pathlib import Path
import argparse
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # æ— GUIåç«¯

def load_reference_curves(golden_dir: Path) -> pd.DataFrame:
    """åŠ è½½å‚è€ƒæ›²çº¿æ•°æ®"""
    curves_path = golden_dir / 'curves.csv'
    return pd.read_csv(curves_path)

def generate_test_curves(reference_df: pd.DataFrame, noise_level: float = 1e-6) -> pd.DataFrame:
    """ç”Ÿæˆæµ‹è¯•æ›²çº¿æ•°æ® (æ¨¡æ‹Ÿå®é™…å®ç°çš„è¾“å‡º)"""
    test_df = reference_df.copy()
    
    # æ·»åŠ å°é‡å™ªå£°æ¨¡æ‹Ÿæ•°å€¼å·®å¼‚
    np.random.seed(42)  # ç¡®ä¿å¯é‡ç°
    noise = np.random.normal(0, noise_level, len(test_df))
    test_df['y'] = test_df['y'] + noise
    
    # ç¡®ä¿èŒƒå›´åˆç†
    test_df['y'] = np.clip(test_df['y'], 0.0, 1.1)
    
    return test_df

def compute_delta_e(ref_curves: pd.DataFrame, test_curves: pd.DataFrame) -> dict:
    """è®¡ç®—Î”Eå·®å¼‚ç»Ÿè®¡"""
    results = {}
    
    for (preset, curve_type), ref_group in ref_curves.groupby(['preset', 'curve_type']):
        test_group = test_curves[
            (test_curves['preset'] == preset) & 
            (test_curves['curve_type'] == curve_type)
        ]
        
        if len(test_group) == 0:
            continue
            
        # ç®€åŒ–çš„Î”Eè®¡ç®— (å®é™…åº”ä½¿ç”¨CIE Î”E00)
        ref_y = ref_group['y'].values
        test_y = test_group['y'].values
        
        delta_e = np.abs(ref_y - test_y) * 100  # è½¬æ¢ä¸ºæ„ŸçŸ¥å•ä½
        
        results[f"{preset}_{curve_type}"] = {
            'mean': float(np.mean(delta_e)),
            'p95': float(np.percentile(delta_e, 95)),
            'p99': float(np.percentile(delta_e, 99)),
            'max': float(np.max(delta_e)),
            'worst_points': find_worst_points(ref_group, test_group, delta_e, top_k=5)
        }
    
    return results

def find_worst_points(ref_group: pd.DataFrame, test_group: pd.DataFrame, 
                     delta_e: np.ndarray, top_k: int = 10) -> list:
    """æ‰¾å‡ºæœ€å·®çš„kä¸ªç‚¹"""
    worst_indices = np.argsort(delta_e)[-top_k:]
    worst_points = []
    
    for idx in worst_indices:
        point = {
            'x': float(ref_group.iloc[idx]['x']),
            'ref_y': float(ref_group.iloc[idx]['y']),
            'test_y': float(test_group.iloc[idx]['y']),
            'delta_e': float(delta_e[idx])
        }
        worst_points.append(point)
    
    return worst_points

def check_monotonicity(curves_df: pd.DataFrame) -> dict:
    """æ£€æŸ¥å•è°ƒæ€§"""
    results = {}
    
    for (preset, curve_type), group in curves_df.groupby(['preset', 'curve_type']):
        y_values = group.sort_values('x')['y'].values
        diff = np.diff(y_values)
        
        monotonic = np.all(diff >= -1e-10)  # å…è®¸æ•°å€¼è¯¯å·®
        min_diff = np.min(diff)
        violations = np.sum(diff < -1e-10)
        
        results[f"{preset}_{curve_type}"] = {
            'monotonic': bool(monotonic),
            'min_diff': float(min_diff),
            'violations': int(violations)
        }
    
    return results

def generate_error_heatmap(delta_e_results: dict, output_path: Path):
    """ç”Ÿæˆè¯¯å·®çƒ­åŠ›å›¾"""
    presets = ['cinema_flat', 'cinema_punch', 'cinema_highlight']
    curves = ['PPR', 'RLOG']
    metrics = ['mean', 'p95', 'p99', 'max']
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    fig.suptitle('Cinema Pro HDR Î”E è¯¯å·®åˆ†æ', fontsize=16)
    
    for i, metric in enumerate(metrics):
        ax = axes[i//2, i%2]
        
        # æ„å»ºçƒ­åŠ›å›¾æ•°æ®
        heatmap_data = np.zeros((len(presets), len(curves)))
        
        for p_idx, preset in enumerate(presets):
            for c_idx, curve in enumerate(curves):
                key = f"{preset}_{curve}"
                if key in delta_e_results:
                    heatmap_data[p_idx, c_idx] = delta_e_results[key][metric]
        
        im = ax.imshow(heatmap_data, cmap='YlOrRd', aspect='auto')
        
        # è®¾ç½®æ ‡ç­¾
        ax.set_xticks(range(len(curves)))
        ax.set_xticklabels(curves)
        ax.set_yticks(range(len(presets)))
        ax.set_yticklabels(presets)
        ax.set_title(f'Î”E {metric.upper()}')
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for p_idx in range(len(presets)):
            for c_idx in range(len(curves)):
                value = heatmap_data[p_idx, c_idx]
                ax.text(c_idx, p_idx, f'{value:.3f}', 
                       ha='center', va='center', color='black')
        
        plt.colorbar(im, ax=ax)
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

def generate_report(delta_e_results: dict, monotonic_results: dict, 
                   output_path: Path) -> bool:
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    
    # æ£€æŸ¥æ˜¯å¦é€šè¿‡è´¨é‡æ ‡å‡†
    passed = True
    issues = []
    
    for key, result in delta_e_results.items():
        if result['mean'] > 0.5:
            passed = False
            issues.append(f"{key}: Î”Eå‡å€¼ {result['mean']:.3f} > 0.5")
        
        if result['p99'] > 1.0:
            passed = False
            issues.append(f"{key}: Î”E 99åˆ†ä½ {result['p99']:.3f} > 1.0")
        
        if result['max'] > 2.0:
            passed = False
            issues.append(f"{key}: Î”Eæœ€å¤§å€¼ {result['max']:.3f} > 2.0")
    
    for key, result in monotonic_results.items():
        if not result['monotonic']:
            passed = False
            issues.append(f"{key}: éå•è°ƒï¼Œ{result['violations']}ä¸ªè¿è§„ç‚¹")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = {
        'timestamp': pd.Timestamp.now().isoformat(),
        'passed': passed,
        'summary': {
            'total_tests': len(delta_e_results),
            'passed_tests': sum(1 for r in delta_e_results.values() 
                              if r['mean'] <= 0.5 and r['p99'] <= 1.0 and r['max'] <= 2.0),
            'monotonic_tests': sum(1 for r in monotonic_results.values() if r['monotonic'])
        },
        'delta_e_results': delta_e_results,
        'monotonic_results': monotonic_results,
        'issues': issues,
        'quality_standards': {
            'delta_e_mean_threshold': 0.5,
            'delta_e_p99_threshold': 1.0,
            'delta_e_max_threshold': 2.0,
            'monotonic_required': True
        }
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    return passed

def main():
    parser = argparse.ArgumentParser(description='Cinema Pro HDR CIå›å½’æµ‹è¯•')
    parser.add_argument('--golden-dir', default='golden', help='å‚è€ƒæ•°æ®ç›®å½•')
    parser.add_argument('--output-dir', default='test_results', help='æµ‹è¯•ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--noise-level', type=float, default=1e-6, help='æµ‹è¯•å™ªå£°æ°´å¹³')
    args = parser.parse_args()
    
    golden_dir = Path(args.golden_dir)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    print("ğŸš€ å¼€å§‹Cinema Pro HDR CIå›å½’æµ‹è¯•...")
    print(f"ğŸ“ å‚è€ƒæ•°æ®: {golden_dir.absolute()}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir.absolute()}")
    print()
    
    # åŠ è½½å‚è€ƒæ›²çº¿
    print("ğŸ“Š åŠ è½½å‚è€ƒæ›²çº¿æ•°æ®...")
    ref_curves = load_reference_curves(golden_dir)
    print(f"âœ… å·²åŠ è½½ {len(ref_curves)} æ¡å‚è€ƒæ›²çº¿è®°å½•")
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ® (æ¨¡æ‹Ÿå®é™…å®ç°è¾“å‡º)
    print(f"ğŸ”§ ç”Ÿæˆæµ‹è¯•æ•°æ® (å™ªå£°æ°´å¹³: {args.noise_level:.2e})...")
    test_curves = generate_test_curves(ref_curves, args.noise_level)
    
    # è®¡ç®—Î”Eå·®å¼‚
    print("ğŸ“ è®¡ç®—Î”Eå·®å¼‚...")
    delta_e_results = compute_delta_e(ref_curves, test_curves)
    
    # æ£€æŸ¥å•è°ƒæ€§
    print("ğŸ“ˆ æ£€æŸ¥å•è°ƒæ€§...")
    monotonic_results = check_monotonicity(test_curves)
    
    # ç”Ÿæˆè¯¯å·®çƒ­åŠ›å›¾
    print("ğŸ¨ ç”Ÿæˆè¯¯å·®çƒ­åŠ›å›¾...")
    heatmap_path = output_dir / 'delta_e_heatmap.png'
    generate_error_heatmap(delta_e_results, heatmap_path)
    
    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    print("ğŸ“‹ ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
    report_path = output_dir / 'regression_test_report.json'
    passed = generate_report(delta_e_results, monotonic_results, report_path)
    
    # è¾“å‡ºç»“æœæ‘˜è¦
    print()
    print("=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦:")
    print()
    
    for key, result in delta_e_results.items():
        status = "âœ…" if (result['mean'] <= 0.5 and result['p99'] <= 1.0 and result['max'] <= 2.0) else "âŒ"
        print(f"{status} {key}:")
        print(f"    Î”Eå‡å€¼: {result['mean']:.3f} (â‰¤0.5)")
        print(f"    Î”E 99åˆ†ä½: {result['p99']:.3f} (â‰¤1.0)")
        print(f"    Î”Eæœ€å¤§å€¼: {result['max']:.3f} (â‰¤2.0)")
        print()
    
    print("ğŸ“ˆ å•è°ƒæ€§æ£€æŸ¥:")
    for key, result in monotonic_results.items():
        status = "âœ…" if result['monotonic'] else "âŒ"
        mono_status = 'å•è°ƒ' if result['monotonic'] else f'éå•è°ƒ ({result["violations"]}ä¸ªè¿è§„ç‚¹)'
        print(f"{status} {key}: {mono_status}")
    
    print()
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  ğŸ“Š è¯¯å·®çƒ­åŠ›å›¾: {heatmap_path}")
    print(f"  ğŸ“‹ æµ‹è¯•æŠ¥å‘Š: {report_path}")
    
    print()
    if passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("âœ… å®ç°ç¬¦åˆè´¨é‡æ ‡å‡†")
        return 0
    else:
        print("ğŸ’¥ æµ‹è¯•å¤±è´¥ï¼")
        print("âŒ å®ç°ä¸ç¬¦åˆè´¨é‡æ ‡å‡†ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é—®é¢˜")
        return 1

if __name__ == '__main__':
    exit(main())